# 3. 머신러닝 평가

목적: 머신러닝
생성일시: 2022년 4월 15일 오전 8:24

---

# 머신러닝 With Python

1. 파이썬 기반 머신러닝 생태계 
2. 사이킥런으로 시작하는 머신러닝 
3. 머신러닝 평가 
4. 머신러닝 지도학습 (분류) 
5. 머신러닝 지도학습 (회귀) 
6. 머신러닝 비지도 학습 (차원축소)
7. 머신러닝 비지도 학습 (군집화) 

---

# 3. 머신러닝 평가

## 3.1. 분류 성능 평가 지표

분류 classification 성능평가 지표 

이진분류에서 주로 성능 평가지표로 활용된다. 

- 정확도 Accuracy
- 오차행렬 (Confusion Martix)
- 정밀도 (Precision)
- 재현율(Recall)
- F1 스코어
- ROC AUC

## 3.2 정확도(accuracy) 문제

정확도란 전체 예측 데이터 건수 중에 예측결과가 동일한 데이터 건수를 말한다. 

즉, 전체 예측 데이터 개수 중에 그중에  잘 예측한 데이터 건수의 비를 말한다고 할 수 있다.  

![Untitled](3%20%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%2024d63/Untitled.png)

## 3.3 오차행렬(Confusion Matrix)

> ****[sklearn.metrics.confusion_matrix¶](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html?highlight=confusion_matrix#sklearn-metrics-confusion-matrix)****
> 
> 
> # **sklearn.metrics.confusion_matrix[¶](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html?highlight=confusion_matrix#sklearn-metrics-confusion-matrix)**
> 
> sklearn.metrics.confusion_matrix(*y_true*, *y_pred*, ***, *labels=None*, *sample_weight=None*, *normalize=None*)[[source]](https://github.com/scikit-learn/scikit-learn/blob/baf828ca1/sklearn/metrics/_classification.py#L222)
> 

> parameter
> 
> 
> **`y_true` *array-like of shape (n_samples,)***
> 
> Ground truth (correct) target values.
> 
> **`y_pred` *array-like of shape (n_samples,)***
> 
> Estimated targets as returned by a classifier.
> 
> **labels*array-like of shape (n_classes), default=None***
> 
> List of labels to index the matrix. This may be used to reorder or select a subset of labels. If `None` is given, those that appear at least once in `y_true` or `y_pred` are used in sorted order.
> 
> **`sample_weight` *array-like of shape (n_samples,), default=None***
> 
> Sample weights.
> *New in version 0.18.*
> 
> **`normalize` *{‘true’, ‘pred’, ‘all’}, default=None***
> 
> Normalizes confusion matrix over the true (rows), predicted (columns) conditions or all the population. If None, confusion matrix will not be normalized.
> 

### 3.3.1 오차행렬을 통한 정확도 지표 문제

|  | 예측 (Positive(1)  | 예측 (NAgative (0)  |  |
| --- | --- | --- | --- |
| 실제 클래스 (True 1)  | TP  | TN  | 재현율: 실제 참 값들 중에 예측해서 맞은 값의 비   |
| 실제 클래스 (Falsse 0 )  | FP  | FN  |  |
|  | 정밀도 : 참이라고 예측한 값 중에 실제로 맞춘 값의 비  |  |  |

### 3.3.2 정밀도와 재현율

- 정확도(Accuracy)
    - **전체 데이터 수** 중 **예측 결과와 실제 값이 동일한 건수**(TN + TP)가 차지하는 비율
    - `(TN + TP) / (TN + FP + FN + TP)`
- 정밀도(Precision)
    - **예측을 Positive로 한 대상**(FP + TP) 중 **예측과 실제 값이 Positive로 일치한 데이터**(TP)의 비율
    - `TP / (FP + TP)`
    - 양성 예측도라고도 불린다.
- 재현율(Recall)
    - **실제가 Positive인 대상**(FN + TP) 중 **예측과 실제 값이 Positive로 일치한 데이터**(TP)의 비율
    - `TP / (FN + TP)`
    - 민감도(Sensitivity), TPR(True Positive Rate)이라고도 불린다.
    

### 3.3.3 정밀도와 재현율 trade off

재현율과 정밀도는 어느정도 상충관계를 이룬다.  

재현율이 질병이 있는데 질병이 있다고 판단한 비를 말하고 

정밀도는 질병이 있다고 예측한 것중 실제로 질병이 있다고 검증된 비를 말한다. 

⇒ 확신보단 일단 아닌 것 같은 애도 분류해야할 때  : 금융사기, 암진단 등등 

만약 암 환자라면 재현율이 높기보다는 정밀도가 높아야 할 것이다. 누가봐도 암환자인 사람을 암환자라고 판단하는 것보단, 암환자가 아닌 것같은데 암환자라고 예측한 사람이 많아야 하기 때문이다. 

⇒ 정말 확신되는 애만 분류해야할 때  : 스팸메일 

반대로 스펨메일을 분류하는 알고리즘은 정밀도 보단 재현율이 높아야 한다. 스펨 아닌 것 같은데 스펨으로 분류해서 휴지통에 넣어버리면 큰일이므로, 누가 봐도 스팸메일 같은 것만 구분해야 하기 때문이다.

- 정밀도/ 재현율의 trade - off
    - 정밀도와 재현율이 강조될 경우 임계값Threshold를 조정해 해당 수치를 조정 가능하다.
    - 상호 보완적인 수치이므로 trade off 작용(상충작용) 이 있다. 따라서 재현율과 정밀도 중 하나의 임계값 threshold 를 강제로 높이면 다른 하나의 수치는 떨어지기 쉽다.

### 3.3.4 분류 결정 임계값(Threshold)에 따른 positiv 예측 확률 변화

- 분류 결정 임계값이 낮아질수록 Positive로 예측할 확률이 높아지고 재현율이 증가한다.
- Estimator 객체의 predit_proba( ) 메소드는 분류 결정 예측 확률을 반환한다. 이를 이용해 임의로 분류 결정 임계값을 조정하면서 예측 확률을 변경할 수 있다.
- 사이킷런은 precision_recall_curve( ) 함수를 통해 임계값에 따른 정밀도, 재현율의 변화값을 제공한다.

![Untitled](3%20%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%2024d63/Untitled%201.png)

> **Predictor**
> 
> 
> [https://scikit-learn.org/stable/developers/develop.html](https://scikit-learn.org/stable/developers/develop.html)
> 
> For supervised learning, or some unsupervised problems, implements:
> `prediction = predictor.predict(data)`
> Classification algorithms usually also offer a way to quantify certainty of a prediction, either using `decision_function` or `predict_proba`:
> `probability = predictor.predict_proba(data)`
> 

### 3.3.5 임계값의 변경에 따른 정밀도 - 재현율 변화 곡선 차트

### 3.3.6 정밀도와 재현율 맹점

- 정밀도를 100%로 만드는 법
    
    확실한 기준이 되는 경우만 positive로 예측하고 나머지는 모두 nagative로 예측
    
    전체 환자 1000명 중 확실한 positive 징후만 가진 환자는 단 1명이라고 하면 이 한명만 positive로 예측하고 나머지는 모두 negative로 예측하더라도 FP (거짓인데 참이라고 예측한 값)은  0이고 TP(참이고 참으로 예측한 값)은 1이 되므로  정밀도 TP /(FP + TP) = 1 , 즉 100%가 된다. 
    
    ⇒ 코로나 환자 1000명중에 무증상환자 999명있고 1명만 유증상일때, 이  유증상 1명만 맞춰도 정밀도 100%**가 나온다**는 것이다. 
    
- 재현율을 100%로 만드는 법
    
    모든 환자를 positive로 예측
    
    전체 환자 **1000명을 다 positive로 예측**하면 재현율 TP/(TP+FN) = 1 , 즉 100%가 된다. 
    
    ⇒ 코로나 환자와 비환자가 섞인 1000명을 전 부 다 코로나 환자로 예측하는 것이다. 실제 코로나 환자가 단 30명만 섞였다고 하더라도 FN(실제로 거짓이고 거짓으로 예측한 값)은 0이 되므로  TN /(TN + FN )  = TN /TN = 1 이 되어버린다. 
    

## 3.4. F1 score

정밀도 + 재현율 지표 

$F1 score = 2TP / 2TP + FN + FN$ 

데이터가 한쪽으로 치우친 경우 재현율은 높게 정밀도는 낮게 나올 수 있다. 이 둘의 평균을 구하고 싶을때 F1 score를 이용한ㄷ다. 

**F1 score가 좋으면 재현율과 정밀도 모두 좋음을 알 수 있다 (평균치가 높다는 것이므로)** 

반면, F1 score가 나브면 재현율과 정밀도 둘 중 누가 나쁜지 백테스팅을 해봐야 한다. (임계값을 다시 잡거나 모델을 다시 고르는 방식의 벡테스팅을 말함) 

> ****[sklearn.metrics.f1_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)****
> 
> 
> # **sklearn.metrics.f1_score**
> 
> sklearn.metrics.f1_score(*y_true*, *y_pred*, ***, *labels=None*, *pos_label=1*, *average='binary'*, *sample_weight=None*, *zero_division='warn'*)[[source]](https://github.com/scikit-learn/scikit-learn/blob/baf828ca1/sklearn/metrics/_classification.py#L992)
> 

> **Parameters**
> 
> 
> **`y_true` *1d array-like, or label indicator array / sparse matrix***
> 
> Ground truth (correct) target values.
> 
> **`y_pred` *1d array-like, or label indicator array / sparse matrix***
> 
> Estimated targets as returned by a classifier.
> 
> **`labels` *array-like, default=None***
> 
> The set of labels to include when `average != 'binary'`, and their order if `average is None`. Labels present in the data can be excluded, for example to calculate a multiclass average ignoring a majority negative class, while labels not present in the data will result in 0 components in a macro average. For multilabel targets, labels are column indices. By default, all labels in `y_true` and `y_pred` are used in sorted order.
> 

## 3.5. ROC 곡선과 AUC

### 3.5.1 ROC 곡선

ROC 곡선 : Receiver Operation Charcterisitc Curve 

가로축이 FP고 세로축이 TP인 값을 함꼐 그린다. ROC곡선은 FPR(flase Positive Rate)이 변할 때, TPR(True Positive Rate)이 어떻게 변하는지를 나타내는 곡선이다. 

TPR : 재현율 

FPR : 특이도 

 

좌표평면상에 x축과 y축을 가로지르는 대각선에 ROC곡선이 가까울수록 안좋은 지표, ROC선이 Y축과 맞닿을수록 좋은 지표라고 할 수 있다. 즉, ROC커브가 급해질 수록 좋다. 

단, 학습에 따라 과적합이 발생하면 이 ROC곡선은 Y축에 거이 맞닿을듯 가까워지다가 다시금 대각선 방향으로 벌어지며 예측률이 떨어지게 된다. 따라서 학습 도중 ROC곡선을 보며 급해진 ROC경사가 완만해지는 순간 직전에 멈추는 early stoping 을 해야한다. 

또한 이 ROC곡선 면적을 AUC라고 한다. ROC가 Y에 맞닿을듯, 경사가 급해지는 것이 좋은 지표인 것과 마찬가지로 이 ROC 경사 밑의 면적인  AUC가 1에 가까울수록 좋은 수치가 된다. 

ROC 활용 

- 데이터 늘려야지
- 알고리즘 바꿔야지
- 학습횟수 늘려야지
- decisiontree라면 10개 경우의수에서 20가지로 파라미터를 늘려야지
- 기타 등등의 예측률을 높이기 위한 파라미터 수정의 지표로 사용

### 3.5.2 사이킥런 ROC 곡선과 AUC score

> ****[sklearn.metrics.roc_curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html?highlight=roc#sklearn.metrics.roc_curve)****
> 
> 
> # **sklearn.metrics.roc_curve**
> 
> sklearn.metrics.roc_curve(*y_true*, *y_score*, ***, *pos_label=None*, *sample_weight=None*, *drop_intermediate=True*)[[source]](https://github.com/scikit-learn/scikit-learn/blob/baf828ca1/sklearn/metrics/_ranking.py#L873)
> 

> **Parameters**
> 
> 
> **`y_true`  : *ndarray of shape (n_samples,)***
> 
> True binary labels. If labels are not either {-1, 1} or {0, 1}, then pos_label should be explicitly given.
> 
> **`y_score` : *ndarray of shape (n_samples,)***
> 
> Target scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of decisions (as returned by “decision_function” on some classifiers).
> 
> **`pos_label` : *int or str, default=None***
> 
> The label of the positive class. When `pos_label=None`, if `y_true` is in {-1, 1} or {0, 1}, `pos_label` is set to 1, otherwise an error will be raised.
> 
> **`sample_weight` : *array-like of shape (n_samples,), default=None***
> 
> Sample weights.
> 
> **`drop_intermediate` : *bool, default=True***
> 
> Whether to drop some suboptimal thresholds which would not appear on a plotted ROC curve. This is useful in order to create lighter ROC curves.
> *New in version 0.17:* parameter *drop_intermediate*.
> 

## 3.6. 머신러닝 평가 실습 _ 인디언 당뇨병 예측

- 
    
    ```python
    
    ```
    
- 
    
    ```python
    
    ```
    

# Reference

-